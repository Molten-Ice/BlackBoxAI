
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: docs/api_data.ipynb

from pathlib import Path
import sys
sys.path.append(Path.cwd().parent.as_posix())

from data.dataset import Dataset
from data.scraper import *

def normalize(x, m, s): return (x-m)/s

def normalize_train_eval(x_train ,x_valid):
    train_mean,train_std = x_train.mean(),x_train.std()
    x_train = normalize(x_train, train_mean, train_std)
    # NB: Use training, not validation mean for validation set
    x_valid = normalize(x_valid, train_mean, train_std)
    return x_train, x_valid

class data():
    @classmethod
    def avaliable_urls(self):
        for key in urls:
            print(key, urls[key])
    @classmethod
    def load_data(self, *args):
        x_train,y_train,x_valid,y_valid = get_data(*args)
        x_train_normalized, x_valid_normalized = normalize_train_eval(x_train ,x_valid)
        train_ds = Dataset(x_train_normalized, y_train)
        valid_ds = Dataset(x_valid_normalized, y_valid)
        c = y_train.max().item()+1
        return train_ds, valid_ds, c